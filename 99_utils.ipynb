{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-junior",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Utility functions used across the application. These methods should not \n",
    "rely on the overall environment. This means that they should in principle\n",
    "focus on implementing functionality with non-application specific code. \n",
    "\n",
    "If when creating methods here you are tapping into the data model or the \n",
    "environment modules ask yourself if the functionality can be refactored to\n",
    "be more general. \n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import hashlib\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import drt.data_model as dm\n",
    "import typing\n",
    "\n",
    "Data_Groups_Type = typing.Union[dm.Delivery, dm.Raw_Data, dm.Dataset]\n",
    "    \n",
    "\n",
    "def hash_files(file_list: List[str], \n",
    "               block_size: int = 10_485_760,\n",
    "               progressbar_min_size: int = 10_737_400_000) -> str:\n",
    "    \"\"\"\n",
    "    Takes a list of path objects and returns the SHA256 hash of\n",
    "    the files in the list. If any of the objects are not file objects,\n",
    "    this will crash. Ignores any files called 'receipt.rst' as those are\n",
    "    considered data intake files and not part of the work.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : List[str]  \n",
    "        List of strings denoting files to be hashed, the strings \n",
    "        must all be valid files or this method will throw a \n",
    "        ValueError exception.\n",
    "\n",
    "    block_size : int, optional  \n",
    "        Block size in bytes to read from disk a good generic \n",
    "        value is 10MB as most files are smaller than 10MB and\n",
    "        it means we can load whole files in at a time when\n",
    "        hashing, defaults to 10_485_760 (10MB).\n",
    "\n",
    "    progressbar_min_size : int, optional  \n",
    "        Minimum size a file needs to be to get its\n",
    "        own progress bar during processing. Default was chosen\n",
    "        to work well on an SSD reading 400 MB/s, defaults \n",
    "        to 10_737_400_000 (10GB).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str  \n",
    "        A string representation of the SHA256 hash of the files\n",
    "        provided in the file list.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError  \n",
    "        The strings passed in the file_list need to be valid file objects\n",
    "        in the file system. Currently only windows and Posix filesystems\n",
    "        are supported. This may change in future.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> files = sorted([f for f in Path.cwd().iterdir() if not f.is_dir()])\n",
    "    >>> hash_files(files)\n",
    "    'ceafa67639bab8e30b0b73955668edec73c1ff2b190db60d74da419144e6c0b0'\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # sort the file list to always give a consistent result. Order matters.\n",
    "    file_list = sorted(file_list)\n",
    "    \n",
    "    file_list_hash = hashlib.sha256()  # Create the hash object for the list\n",
    "\n",
    "    # loop through all the files in the list updating the hash object\n",
    "    for fil in tqdm(file_list, leave=False, unit=\"files\"):\n",
    "        file_progress_bar = False\n",
    "\n",
    "        if not Path(fil).exists():\n",
    "            raise FileNotFoundError(\"Strings in the file_list must be a valid path in the filesystem\")\n",
    "        elif Path(fil).is_dir():\n",
    "            raise ValueError(\"Strings in the file_list must be a valid files not folders\")\n",
    "\n",
    "        size = fil.stat().st_size\n",
    "        if size > progressbar_min_size:\n",
    "            file_progress_bar = True\n",
    "            pbar = tqdm(total=fil.stat().st_size, unit=\"bytes\", unit_scale=True, leave=False)\n",
    "        else:\n",
    "            pbar = [] # else only here to get rid of unbound warning\n",
    "\n",
    "        # Read data from file in block_size chunks and update the folder hash function\n",
    "        with open(fil, \"rb\") as f:\n",
    "            fb = f.read(block_size)\n",
    "            while len(fb) > 0:\n",
    "                file_list_hash.update(fb)  # Update the file list hash\n",
    "                fb = f.read(block_size)  # Read the next block from the file\n",
    "                if file_progress_bar:\n",
    "                    pbar.update(block_size)\n",
    "        if file_progress_bar:\n",
    "            pbar.close()\n",
    "    return file_list_hash.hexdigest()\n",
    "\n",
    "def process_data_group(folder:Path, type:str, light:bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Return the system fields for a data group folder. \n",
    "\n",
    "    If the data group is a delivery type, then this only looks at the\n",
    "    data folder in it, if it is any other type it looks at the whole folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : Path\n",
    "        The location to get metadata for.\n",
    "        \n",
    "    type : DataIntakeEnv\n",
    "        The type of data group. ['delivery', 'raw_data', 'dataset']\n",
    "        \n",
    "    light : bool, optional\n",
    "        If set skip the hashing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dict of the following five metadata elements calculated:\n",
    "        - name : Name of the folder of the data group\n",
    "        - type : The type of the data group processed\n",
    "        - last_update : The current date and time\n",
    "        - size : The size of the data on disk\n",
    "        - num_files : The number of data files. \n",
    "        - group_hash : A SHA256 hash of all the data in the folder\n",
    "        - group_last_modified : The maximum date of created, and modified for all files\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> process_data_group(delivery, dm.Delivery)\n",
    "    {\n",
    "        \"name\" : '2020_11_01_test_delivery',\n",
    "        \"type\" : 'delivery',\n",
    "        \"last_update\" : datetime.datetime(2020, 11, 15, 3, 51, 33, 851325),\n",
    "        \"size\" : 5465684,\n",
    "        \"num_files\" : 4,\n",
    "        \"group_hash\" : 'ceafa67639bab8e30b0b73955668edec73c1ff2b190db60d74da419144e6c0b0',\n",
    "        \"group_last_modified\" : datetime.datetime(2020, 11, 15, 3, 31, 36),\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    if type == dm.Delivery:\n",
    "        data_folder = folder / 'data'\n",
    "    else: \n",
    "        data_folder = folder\n",
    "\n",
    "    # check for non-existent or empty folder\n",
    "    if not data_folder.exists():\n",
    "        raise FileNotFoundError\n",
    "    try:\n",
    "        next((data_folder).glob(\"**/*\"))\n",
    "    except StopIteration:\n",
    "        # folder is empty can't process it\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    # Get file sizes, last modified dates, and names to count,\n",
    "    # sum size, and hash the file data provided\n",
    "    file_sizes, file_modified_dates, file_metamodified_dates, file_names = zip(\n",
    "        *[\n",
    "            (f.stat().st_size, f.stat().st_mtime, f.stat().st_ctime, f)\n",
    "            for f in (data_folder).glob(\"**/*\")\n",
    "            if f.is_file() and f.name != 'receipt.rst' \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    last_modified = datetime.fromtimestamp(\n",
    "                    max(max(file_modified_dates), \n",
    "                        max(file_metamodified_dates)))\n",
    "\n",
    "    # Hash the files in the delivery\n",
    "    if light:\n",
    "        folder_hash = 'skipped'\n",
    "    else:\n",
    "        folder_hash = hash_files(file_names)\n",
    "    \n",
    "    dg = {\n",
    "     'name' : folder.name,\n",
    "     'type' : type.__name__,\n",
    "     'last_update' : datetime.now(),\n",
    "     'size' : sum(file_sizes),\n",
    "     'num_files' : len(file_sizes),\n",
    "     'group_hash' : folder_hash,\n",
    "     'group_last_modified' : last_modified,\n",
    "    }\n",
    "\n",
    "    return dg\n",
    "\n",
    "def count_data_group_components( data_group: Path,\n",
    "                    data_extensions: list,\n",
    "                    report_extensions: list,\n",
    "                    script_extensions: list,\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    A utility method to analyze a folder to determine which data\n",
    "    it contains and whether those have the three requisite elements,\n",
    "    generation script, data, and report. It relies on certain \n",
    "    conventions about the folder which must be followed:\n",
    "\n",
    "    1. Each data respresentation is stored in a folder, files in \n",
    "        the root of the passed folder will be ignored.\n",
    "    2. Folders starting with \"In_Progress\" or \".\" will be ignored.\n",
    "    3. In each data representation folder there are three entries\n",
    "        more won't cause an error but should be avoided\n",
    "    4. Report types have extensions: \n",
    "            ['report','md','html','pptx','docx', ...]\n",
    "        with the initial report extension added to a folder containing\n",
    "        report files if there is more than 1 report file needed.\n",
    "    5. Data types have extensions:\n",
    "            ['data','parquet','hdf5', ...]\n",
    "        with the initial data extension being used for folders in the \n",
    "        declaration which allows the data to be spread over multiple \n",
    "        files. \n",
    "    6. Script types have extensions:\n",
    "            ['script','ipynb','py','r','jl','sh', ...]\n",
    "        Where the first extension can be applied to a folder if more \n",
    "        than one file was needed to process the data. \n",
    "\n",
    "    This analyzer will look only for the extensions listed and report\n",
    "    how many of each of the types of files/folders exist in the root \n",
    "    of the provided folder.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : Path\n",
    "        A folder containing folders of data representations\n",
    "        \n",
    "    TODO add extension list parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A table listing all the data representations which appear\n",
    "        in the root of the folder.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> analyze_folder(folder)\n",
    "    {\n",
    "        'data'   : 1,\n",
    "        'report' : 0,\n",
    "        'script' : 1\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    element_count ={\n",
    "        'data':0,\n",
    "        'report':0,\n",
    "        'script':0\n",
    "    }\n",
    "\n",
    "    # For each Raw data file extract count the number of each data elements it has\n",
    "    for fil in data_group.iterdir():\n",
    "        if not fil.name.startswith('.'):\n",
    "            if fil.suffix in data_extensions:\n",
    "                element_count['data'] += 1\n",
    "            if fil.suffix in report_extensions:\n",
    "                element_count['report'] += 1\n",
    "            if fil.suffix in script_extensions:\n",
    "                element_count['script'] += 1  \n",
    "\n",
    "    return element_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drt",
   "language": "python",
   "name": "drt"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
